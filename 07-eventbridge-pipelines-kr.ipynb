{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "575c15ae-ffe0-4d93-84d8-bf96278c0372",
   "metadata": {
    "tags": []
   },
   "source": [
    "# 7단계: EventBridge 기반 자동화 파이프라인\n",
    "\n",
    "<div class=\"alert alert-warning\"> 이 노트북은 <code>SageMaker Distribution Image 3.6.1</code>을 사용하는 SageMaker Studio JupyterLab 인스턴스에서 SageMaker Python SDK 버전 <code>2.255.0</code>으로 마지막으로 테스트되었습니다</div>\n",
    "\n",
    "이 단계에서는 Amazon EventBridge를 사용하여 S3 이벤트 기반으로 자동 트리거되는 두 개의 파이프라인을 구축합니다:\n",
    "\n",
    "1. **Batch Inference Pipeline**: S3에 새로운 데이터가 업로드되면 자동으로 전처리 및 배치 추론을 수행\n",
    "2. **Training Pipeline**: 배치 추론 결과가 S3에 저장되면 자동으로 모델 재훈련 및 등록을 수행\n",
    "\n",
    "||||\n",
    "|---|---|---|\n",
    "|1. |노트북에서 실험 ||\n",
    "|2. |SageMaker AI 처리 작업 및 SageMaker SDK로 확장 ||\n",
    "|3. |ML 파이프라인, 모델 레지스트리 및 피처 스토어로 운영화 ||\n",
    "|4. |모델 구축 CI/CD 파이프라인 추가 ||\n",
    "|5. |모델 배포 파이프라인 추가 ||\n",
    "|6. |모델 및 데이터 모니터링 추가 ||\n",
    "|7. |EventBridge 기반 자동화 파이프라인 |**<<<< 현재 위치**|\n",
    "\n",
    "\n",
    "<div class=\"alert alert-info\"> 이 노트북에서는 JupyterLab에서 <code>Python 3</code> 커널을 사용하고 있는지 확인하세요.</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "401790be-c212-4cb1-b265-4fb0e739836a",
   "metadata": {},
   "source": [
    "## 아키텍처 개요\n",
    "\n",
    "```\n",
    "S3 Input Data Upload\n",
    "        ↓\n",
    "    EventBridge\n",
    "        ↓\n",
    "Batch Inference Pipeline\n",
    "    ↓ (Processing) → (Batch Transform)\n",
    "        ↓\n",
    "S3 Inference Results\n",
    "        ↓\n",
    "    EventBridge\n",
    "        ↓\n",
    "Training Pipeline\n",
    "    ↓ (Preprocessing) → (Training) → (Evaluation & Registration)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0fd46bfb-a2b3-42d4-a136-616ddcd12be5",
   "metadata": {},
   "source": [
    "## 환경 설정 및 라이브러리 가져오기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ca42d3f-7231-44cc-83f4-a54c6a7995f4",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import boto3\n",
    "import sagemaker\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import json\n",
    "import time\n",
    "from datetime import datetime\n",
    "from sagemaker import get_execution_role\n",
    "from sagemaker.workflow.pipeline import Pipeline\n",
    "from sagemaker.workflow.steps import ProcessingStep, TrainingStep, CreateModelStep\n",
    "from sagemaker.workflow.step_collections import RegisterModel\n",
    "from sagemaker.workflow.parameters import ParameterString, ParameterInteger\n",
    "from sagemaker.workflow.properties import PropertyFile\n",
    "from sagemaker.workflow.conditions import ConditionGreaterThanOrEqualTo\n",
    "from sagemaker.workflow.condition_step import ConditionStep\n",
    "from sagemaker.processing import ProcessingInput, ProcessingOutput\n",
    "from sagemaker.sklearn.processing import SKLearnProcessor\n",
    "from sagemaker.xgboost.estimator import XGBoost\n",
    "from sagemaker.transformer import Transformer\n",
    "from sagemaker.model import Model\n",
    "from sagemaker.inputs import TrainingInput\n",
    "\n",
    "print(f\"SageMaker version: {sagemaker.__version__}\")\n",
    "print(f\"Boto3 version: {boto3.__version__}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1cc5659e-ab01-4704-8063-1bdf3ce532ce",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# SageMaker 세션 및 역할 설정\n",
    "sagemaker_session = sagemaker.Session()\n",
    "role = get_execution_role()\n",
    "region = sagemaker_session.boto_region_name\n",
    "bucket = sagemaker_session.default_bucket()\n",
    "prefix = 'eventbridge-pipelines'\n",
    "\n",
    "# AWS 클라이언트 초기화\n",
    "s3_client = boto3.client('s3')\n",
    "events_client = boto3.client('events')\n",
    "iam_client = boto3.client('iam')\n",
    "lambda_client = boto3.client('lambda')\n",
    "\n",
    "print(f\"Region: {region}\")\n",
    "print(f\"Role: {role}\")\n",
    "print(f\"Default bucket: {bucket}\")\n",
    "print(f\"Prefix: {prefix}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dbf33079-51c6-4270-b03c-bb39bfe9bb0f",
   "metadata": {},
   "source": [
    "## S3 버킷 구조 설정\n",
    "\n",
    "파이프라인에 필요한 S3 경로들을 정의합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae1d8226",
   "metadata": {},
   "outputs": [],
   "source": [
    "# S3 경로 정의\n",
    "input_data_path = f\"s3://{bucket}/{prefix}/input-data\"\n",
    "batch_inference_input_path = f\"s3://{bucket}/{prefix}/batch-inference/input\"\n",
    "batch_inference_output_path = f\"s3://{bucket}/{prefix}/batch-inference/output\"\n",
    "training_data_path = f\"s3://{bucket}/{prefix}/training-data\"\n",
    "model_artifacts_path = f\"s3://{bucket}/{prefix}/model-artifacts\"\n",
    "processing_code_path = f\"s3://{bucket}/{prefix}/code\"\n",
    "\n",
    "print(\"S3 경로 설정:\")\n",
    "print(f\"입력 데이터: {input_data_path}\")\n",
    "print(f\"배치 추론 입력: {batch_inference_input_path}\")\n",
    "print(f\"배치 추론 출력: {batch_inference_output_path}\")\n",
    "print(f\"훈련 데이터: {training_data_path}\")\n",
    "print(f\"모델 아티팩트: {model_artifacts_path}\")\n",
    "print(f\"처리 코드: {processing_code_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f0ef893-66d0-4b81-9dfb-d18a441b3e4d",
   "metadata": {},
   "source": [
    "## 전처리 스크립트 생성\n",
    "\n",
    "배치 추론과 훈련에 사용할 전처리 스크립트를 생성합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89f8d9fa-0d8f-4645-a42b-6fe9447355e5",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": [
     "parameters"
    ]
   },
   "outputs": [],
   "source": [
    "%%writefile preprocessing.py\n",
    "\n",
    "import argparse\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "import joblib\n",
    "\n",
    "def preprocess_data(input_path, output_path, mode='inference'):\n",
    "    \"\"\"\n",
    "    데이터 전처리 함수\n",
    "    mode: 'inference' 또는 'training'\n",
    "    \"\"\"\n",
    "    # 데이터 로드\n",
    "    df = pd.read_csv(os.path.join(input_path, 'data.csv'))\n",
    "    \n",
    "    # 기본 전처리\n",
    "    # 범주형 변수 인코딩\n",
    "    categorical_columns = df.select_dtypes(include=['object']).columns\n",
    "    \n",
    "    if mode == 'training':\n",
    "        # 훈련 모드: 레이블 인코더 생성 및 저장\n",
    "        label_encoders = {}\n",
    "        \n",
    "        for col in categorical_columns:\n",
    "            if col != 'y':  # 타겟 변수 제외\n",
    "                le = LabelEncoder()\n",
    "                df[col] = le.fit_transform(df[col].astype(str))\n",
    "                label_encoders[col] = le\n",
    "        \n",
    "        # 레이블 인코더 저장\n",
    "        joblib.dump(label_encoders, os.path.join(output_path, 'label_encoders.pkl'))\n",
    "        \n",
    "        # 타겟 변수 처리 (이진 분류)\n",
    "        if 'y' in df.columns:\n",
    "            df['y'] = (df['y'] == 'yes').astype(int)\n",
    "        \n",
    "        # 훈련/검증 분할\n",
    "        train_df, val_df = train_test_split(df, test_size=0.2, random_state=42)\n",
    "        \n",
    "        # 저장\n",
    "        train_df.to_csv(os.path.join(output_path, 'train.csv'), index=False, header=False)\n",
    "        val_df.to_csv(os.path.join(output_path, 'validation.csv'), index=False, header=False)\n",
    "        \n",
    "    else:\n",
    "        # 추론 모드: 기존 레이블 인코더 로드\n",
    "        try:\n",
    "            label_encoders = joblib.load('/opt/ml/processing/model/label_encoders.pkl')\n",
    "            \n",
    "            for col in categorical_columns:\n",
    "                if col in label_encoders:\n",
    "                    # 새로운 카테고리 처리\n",
    "                    le = label_encoders[col]\n",
    "                    df[col] = df[col].astype(str)\n",
    "                    \n",
    "                    # 알려진 클래스만 변환\n",
    "                    mask = df[col].isin(le.classes_)\n",
    "                    df.loc[mask, col] = le.transform(df.loc[mask, col])\n",
    "                    df.loc[~mask, col] = -1  # 새로운 카테고리는 -1로 설정\n",
    "                    \n",
    "        except FileNotFoundError:\n",
    "            print(\"레이블 인코더를 찾을 수 없습니다. 기본 처리를 수행합니다.\")\n",
    "            for col in categorical_columns:\n",
    "                df[col] = pd.Categorical(df[col]).codes\n",
    "        \n",
    "        # 타겟 변수 제거 (추론용)\n",
    "        if 'y' in df.columns:\n",
    "            df = df.drop('y', axis=1)\n",
    "        \n",
    "        # 저장\n",
    "        df.to_csv(os.path.join(output_path, 'inference_data.csv'), index=False, header=False)\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    parser = argparse.ArgumentParser()\n",
    "    parser.add_argument('--input-path', type=str, default='/opt/ml/processing/input')\n",
    "    parser.add_argument('--output-path', type=str, default='/opt/ml/processing/output')\n",
    "    parser.add_argument('--mode', type=str, default='inference', choices=['inference', 'training'])\n",
    "    \n",
    "    args = parser.parse_args()\n",
    "    \n",
    "    preprocess_data(args.input_path, args.output_path, args.mode)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "573dc870-cf00-4083-b9d9-d274dc39d89a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# 전처리 스크립트를 S3에 업로드\n",
    "preprocessing_script_uri = sagemaker_session.upload_data(\n",
    "    path='preprocessing.py',\n",
    "    bucket=bucket,\n",
    "    key_prefix=f'{prefix}/code'\n",
    ")\n",
    "print(f\"전처리 스크립트 업로드 완료: {preprocessing_script_uri}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "batch-inference-pipeline",
   "metadata": {},
   "source": [
    "## 1. Batch Inference Pipeline 생성\n",
    "\n",
    "S3에 새로운 데이터가 업로드되면 자동으로 실행되는 배치 추론 파이프라인을 생성합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "batch-inference-params",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 배치 추론 파이프라인 파라미터 정의\n",
    "batch_input_data = ParameterString(\n",
    "    name=\"BatchInputData\",\n",
    "    default_value=batch_inference_input_path\n",
    ")\n",
    "\n",
    "batch_output_data = ParameterString(\n",
    "    name=\"BatchOutputData\", \n",
    "    default_value=batch_inference_output_path\n",
    ")\n",
    "\n",
    "model_name = ParameterString(\n",
    "    name=\"ModelName\",\n",
    "    default_value=\"xgboost-model\"\n",
    ")\n",
    "\n",
    "instance_type = ParameterString(\n",
    "    name=\"InstanceType\",\n",
    "    default_value=\"ml.m5.large\"\n",
    ")\n",
    "\n",
    "instance_count = ParameterInteger(\n",
    "    name=\"InstanceCount\",\n",
    "    default_value=1\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "batch-preprocessing-step",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 배치 추론용 전처리 스텝\n",
    "sklearn_processor = SKLearnProcessor(\n",
    "    framework_version=\"1.2-1\",\n",
    "    instance_type=\"ml.m5.large\",\n",
    "    instance_count=1,\n",
    "    base_job_name=\"batch-preprocessing\",\n",
    "    role=role,\n",
    ")\n",
    "\n",
    "batch_preprocessing_step = ProcessingStep(\n",
    "    name=\"BatchPreprocessing\",\n",
    "    processor=sklearn_processor,\n",
    "    inputs=[\n",
    "        ProcessingInput(\n",
    "            source=batch_input_data,\n",
    "            destination=\"/opt/ml/processing/input\"\n",
    "        ),\n",
    "        ProcessingInput(\n",
    "            source=f\"{processing_code_path}/label_encoders.pkl\",\n",
    "            destination=\"/opt/ml/processing/model\"\n",
    "        )\n",
    "    ],\n",
    "    outputs=[\n",
    "        ProcessingOutput(\n",
    "            output_name=\"preprocessed_data\",\n",
    "            source=\"/opt/ml/processing/output\",\n",
    "            destination=f\"{batch_inference_input_path}/preprocessed\"\n",
    "        )\n",
    "    ],\n",
    "    code=preprocessing_script_uri,\n",
    "    job_arguments=[\"--mode\", \"inference\"]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "batch-transform-step",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 배치 변환 스텝 (추론)\n",
    "# 기존 모델을 사용한다고 가정 (모델 레지스트리에서 가져오거나 S3에서 로드)\n",
    "xgb_model_uri = f\"{model_artifacts_path}/xgboost-model.tar.gz\"  # 기존 모델 경로\n",
    "\n",
    "# XGBoost 모델 생성\n",
    "xgb_model = Model(\n",
    "    image_uri=sagemaker.image_uris.retrieve(\"xgboost\", region, version=\"1.7-1\"),\n",
    "    model_data=xgb_model_uri,\n",
    "    role=role,\n",
    "    sagemaker_session=sagemaker_session\n",
    ")\n",
    "\n",
    "# Transformer 생성\n",
    "transformer = Transformer(\n",
    "    model_name=model_name,\n",
    "    instance_count=instance_count,\n",
    "    instance_type=instance_type,\n",
    "    output_path=batch_output_data,\n",
    "    sagemaker_session=sagemaker_session\n",
    ")\n",
    "\n",
    "# 배치 변환 스텝은 SageMaker Pipelines에서 직접 지원하지 않으므로\n",
    "# Lambda 함수를 통해 실행하거나 별도의 스텝으로 구현\n",
    "print(\"배치 변환 설정 완료\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "batch-pipeline-creation",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 배치 추론 파이프라인 생성\n",
    "batch_pipeline = Pipeline(\n",
    "    name=\"batch-inference-pipeline\",\n",
    "    parameters=[\n",
    "        batch_input_data,\n",
    "        batch_output_data,\n",
    "        model_name,\n",
    "        instance_type,\n",
    "        instance_count\n",
    "    ],\n",
    "    steps=[\n",
    "        batch_preprocessing_step,\n",
    "        # 배치 변환은 Lambda 함수로 처리\n",
    "    ],\n",
    "    sagemaker_session=sagemaker_session\n",
    ")\n",
    "\n",
    "print(\"배치 추론 파이프라인 정의 완료\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "training-pipeline",
   "metadata": {},
   "source": [
    "## 2. Training Pipeline 생성\n",
    "\n",
    "배치 추론 결과가 S3에 저장되면 자동으로 실행되는 훈련 파이프라인을 생성합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "training-params",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 훈련 파이프라인 파라미터 정의\n",
    "training_input_data = ParameterString(\n",
    "    name=\"TrainingInputData\",\n",
    "    default_value=training_data_path\n",
    ")\n",
    "\n",
    "model_approval_status = ParameterString(\n",
    "    name=\"ModelApprovalStatus\",\n",
    "    default_value=\"PendingManualApproval\"\n",
    ")\n",
    "\n",
    "accuracy_threshold = ParameterString(\n",
    "    name=\"AccuracyThreshold\",\n",
    "    default_value=\"0.7\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "training-preprocessing-step",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 훈련용 전처리 스텝\n",
    "training_preprocessing_step = ProcessingStep(\n",
    "    name=\"TrainingPreprocessing\",\n",
    "    processor=sklearn_processor,\n",
    "    inputs=[\n",
    "        ProcessingInput(\n",
    "            source=training_input_data,\n",
    "            destination=\"/opt/ml/processing/input\"\n",
    "        )\n",
    "    ],\n",
    "    outputs=[\n",
    "        ProcessingOutput(\n",
    "            output_name=\"train_data\",\n",
    "            source=\"/opt/ml/processing/output\",\n",
    "            destination=f\"{training_data_path}/processed\"\n",
    "        )\n",
    "    ],\n",
    "    code=preprocessing_script_uri,\n",
    "    job_arguments=[\"--mode\", \"training\"]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "training-step",
   "metadata": {},
   "outputs": [],
   "source": [
    "# XGBoost 훈련 스텝\n",
    "xgb_estimator = XGBoost(\n",
    "    entry_point=\"train.py\",\n",
    "    source_dir=\"code\",  # 훈련 스크립트가 있는 디렉토리\n",
    "    framework_version=\"1.7-1\",\n",
    "    instance_type=\"ml.m5.large\",\n",
    "    instance_count=1,\n",
    "    role=role,\n",
    "    base_job_name=\"xgboost-training\",\n",
    "    hyperparameters={\n",
    "        \"max_depth\": 5,\n",
    "        \"eta\": 0.2,\n",
    "        \"gamma\": 4,\n",
    "        \"min_child_weight\": 6,\n",
    "        \"subsample\": 0.8,\n",
    "        \"objective\": \"binary:logistic\",\n",
    "        \"num_round\": 100\n",
    "    }\n",
    ")\n",
    "\n",
    "training_step = TrainingStep(\n",
    "    name=\"XGBoostTraining\",\n",
    "    estimator=xgb_estimator,\n",
    "    inputs={\n",
    "        \"train\": TrainingInput(\n",
    "            s3_data=training_preprocessing_step.properties.ProcessingOutputConfig.Outputs[\n",
    "                \"train_data\"\n",
    "            ].S3Output.S3Uri + \"/train.csv\",\n",
    "            content_type=\"text/csv\"\n",
    "        ),\n",
    "        \"validation\": TrainingInput(\n",
    "            s3_data=training_preprocessing_step.properties.ProcessingOutputConfig.Outputs[\n",
    "                \"train_data\"\n",
    "            ].S3Output.S3Uri + \"/validation.csv\",\n",
    "            content_type=\"text/csv\"\n",
    "        )\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "evaluation-script",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile evaluation.py\n",
    "\n",
    "import argparse\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import joblib\n",
    "import json\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, roc_auc_score\n",
    "\n",
    "def evaluate_model(model_path, test_data_path, output_path):\n",
    "    # 테스트 데이터 로드\n",
    "    test_df = pd.read_csv(os.path.join(test_data_path, 'validation.csv'), header=None)\n",
    "    \n",
    "    # 특성과 타겟 분리 (첫 번째 열이 타겟)\n",
    "    X_test = test_df.iloc[:, 1:].values\n",
    "    y_test = test_df.iloc[:, 0].values\n",
    "    \n",
    "    # 간단한 평가 (실제로는 모델을 로드해서 예측해야 함)\n",
    "    # 여기서는 예시로 랜덤 예측 사용\n",
    "    np.random.seed(42)\n",
    "    y_pred = np.random.randint(0, 2, len(y_test))\n",
    "    y_pred_proba = np.random.random(len(y_test))\n",
    "    \n",
    "    # 메트릭 계산\n",
    "    accuracy = accuracy_score(y_test, y_pred)\n",
    "    \n",
    "    # 결과 저장\n",
    "    evaluation_results = {\n",
    "        \"accuracy\": float(accuracy)\n",
    "    }\n",
    "    \n",
    "    # JSON 파일로 저장\n",
    "    with open(os.path.join(output_path, 'evaluation.json'), 'w') as f:\n",
    "        json.dump(evaluation_results, f)\n",
    "    \n",
    "    print(f\"모델 평가 완료: {evaluation_results}\")\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    parser = argparse.ArgumentParser()\n",
    "    parser.add_argument('--model-path', type=str, default='/opt/ml/processing/model')\n",
    "    parser.add_argument('--test-data-path', type=str, default='/opt/ml/processing/test')\n",
    "    parser.add_argument('--output-path', type=str, default='/opt/ml/processing/evaluation')\n",
    "    \n",
    "    args = parser.parse_args()\n",
    "    \n",
    "    evaluate_model(args.model_path, args.test_data_path, args.output_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "deploy-pipelines",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 평가 스크립트 업로드\n",
    "evaluation_script_uri = sagemaker_session.upload_data(\n",
    "    path='evaluation.py',\n",
    "    bucket=bucket,\n",
    "    key_prefix=f'{prefix}/code'\n",
    ")\n",
    "\n",
    "# 모델 평가 스텝\n",
    "evaluation_processor = SKLearnProcessor(\n",
    "    framework_version=\"1.2-1\",\n",
    "    instance_type=\"ml.m5.large\",\n",
    "    instance_count=1,\n",
    "    base_job_name=\"model-evaluation\",\n",
    "    role=role,\n",
    ")\n",
    "\n",
    "evaluation_step = ProcessingStep(\n",
    "    name=\"ModelEvaluation\",\n",
    "    processor=evaluation_processor,\n",
    "    inputs=[\n",
    "        ProcessingInput(\n",
    "            source=training_step.properties.ModelArtifacts.S3ModelArtifacts,\n",
    "            destination=\"/opt/ml/processing/model\"\n",
    "        ),\n",
    "        ProcessingInput(\n",
    "            source=training_preprocessing_step.properties.ProcessingOutputConfig.Outputs[\n",
    "                \"train_data\"\n",
    "            ].S3Output.S3Uri,\n",
    "            destination=\"/opt/ml/processing/test\"\n",
    "        )\n",
    "    ],\n",
    "    outputs=[\n",
    "        ProcessingOutput(\n",
    "            output_name=\"evaluation\",\n",
    "            source=\"/opt/ml/processing/evaluation\"\n",
    "        )\n",
    "    ],\n",
    "    code=evaluation_script_uri\n",
    ")\n",
    "\n",
    "# 평가 결과 속성 파일\n",
    "evaluation_report = PropertyFile(\n",
    "    name=\"EvaluationReport\",\n",
    "    output_name=\"evaluation\",\n",
    "    path=\"evaluation.json\"\n",
    ")\n",
    "evaluation_step.add_property_file(evaluation_report)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "model-registration",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 모델 등록 스텝\n",
    "model_package_group_name = \"xgboost-model-package-group\"\n",
    "\n",
    "register_model_step = RegisterModel(\n",
    "    name=\"RegisterXGBoostModel\",\n",
    "    estimator=xgb_estimator,\n",
    "    model_data=training_step.properties.ModelArtifacts.S3ModelArtifacts,\n",
    "    content_types=[\"text/csv\"],\n",
    "    response_types=[\"text/csv\"],\n",
    "    inference_instances=[\"ml.t2.medium\", \"ml.m5.large\"],\n",
    "    transform_instances=[\"ml.m5.large\"],\n",
    "    model_package_group_name=model_package_group_name,\n",
    "    approval_status=model_approval_status\n",
    ")\n",
    "\n",
    "# 조건부 스텝 (정확도 임계값 확인)\n",
    "accuracy_condition = ConditionGreaterThanOrEqualTo(\n",
    "    left=evaluation_report.get(\"accuracy\"),\n",
    "    right=accuracy_threshold\n",
    ")\n",
    "\n",
    "condition_step = ConditionStep(\n",
    "    name=\"AccuracyCondition\",\n",
    "    conditions=[accuracy_condition],\n",
    "    if_steps=[register_model_step],\n",
    "    else_steps=[]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "training-pipeline-creation",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 훈련 파이프라인 생성\n",
    "training_pipeline = Pipeline(\n",
    "    name=\"training-pipeline\",\n",
    "    parameters=[\n",
    "        training_input_data,\n",
    "        model_approval_status,\n",
    "        accuracy_threshold\n",
    "    ],\n",
    "    steps=[\n",
    "        training_preprocessing_step,\n",
    "        training_step,\n",
    "        evaluation_step,\n",
    "        condition_step\n",
    "    ],\n",
    "    sagemaker_session=sagemaker_session\n",
    ")\n",
    "\n",
    "print(\"훈련 파이프라인 정의 완료\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eventbridge-setup",
   "metadata": {},
   "source": [
    "## 3. EventBridge 및 Lambda 설정\n",
    "\n",
    "S3 이벤트를 기반으로 파이프라인을 자동 실행하는 EventBridge 규칙과 Lambda 함수를 설정합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "pipeline-deployment",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 파이프라인 배포\n",
    "print(\"배치 추론 파이프라인 배포 중...\")\n",
    "batch_pipeline.upsert(role_arn=role)\n",
    "print(\"배치 추론 파이프라인 배포 완료\")\n",
    "\n",
    "print(\"훈련 파이프라인 배포 중...\")\n",
    "training_pipeline.upsert(role_arn=role)\n",
    "print(\"훈련 파이프라인 배포 완료\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "test-pipeline",
   "metadata": {},
   "outputs": [],
   "source": [
    "## 4. 테스트 및 실행\n",
    "\n",
    "# 샘플 데이터 생성 및 업로드 (테스트용)\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# 샘플 데이터 생성\n",
    "np.random.seed(42)\n",
    "sample_data = pd.DataFrame({\n",
    "    'age': np.random.randint(18, 80, 100),\n",
    "    'job': np.random.choice(['admin', 'technician', 'services', 'management'], 100),\n",
    "    'marital': np.random.choice(['married', 'single', 'divorced'], 100),\n",
    "    'education': np.random.choice(['primary', 'secondary', 'tertiary'], 100),\n",
    "    'balance': np.random.randint(-1000, 5000, 100),\n",
    "    'housing': np.random.choice(['yes', 'no'], 100),\n",
    "    'loan': np.random.choice(['yes', 'no'], 100),\n",
    "    'duration': np.random.randint(0, 1000, 100),\n",
    "    'campaign': np.random.randint(1, 10, 100),\n",
    "    'y': np.random.choice(['yes', 'no'], 100)\n",
    "})\n",
    "\n",
    "# 테스트 데이터 저장\n",
    "sample_data.to_csv('sample_input_data.csv', index=False)\n",
    "sample_data.to_csv('sample_training_data.csv', index=False)\n",
    "\n",
    "print(\"샘플 데이터 생성 완료\")\n",
    "print(sample_data.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "upload-test-data",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 테스트 데이터 업로드\n",
    "input_data_uri = sagemaker_session.upload_data(\n",
    "    path='sample_input_data.csv',\n",
    "    bucket=bucket,\n",
    "    key_prefix=f'{prefix}/input-data'\n",
    ")\n",
    "\n",
    "training_data_uri = sagemaker_session.upload_data(\n",
    "    path='sample_training_data.csv', \n",
    "    bucket=bucket,\n",
    "    key_prefix=f'{prefix}/training-data'\n",
    ")\n",
    "\n",
    "print(f\"입력 데이터 업로드: {input_data_uri}\")\n",
    "print(f\"훈련 데이터 업로드: {training_data_uri}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "manual-pipeline-test",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 수동으로 파이프라인 실행 테스트\n",
    "print(\"배치 추론 파이프라인 수동 실행...\")\n",
    "batch_execution = batch_pipeline.start(\n",
    "    parameters={\n",
    "        \"BatchInputData\": f\"s3://{bucket}/{prefix}/input-data\"\n",
    "    }\n",
    ")\n",
    "print(f\"배치 추론 파이프라인 실행 ARN: {batch_execution.arn}\")\n",
    "\n",
    "print(\"\\n훈련 파이프라인 수동 실행...\")\n",
    "training_execution = training_pipeline.start(\n",
    "    parameters={\n",
    "        \"TrainingInputData\": f\"s3://{bucket}/{prefix}/training-data\"\n",
    "    }\n",
    ")\n",
    "print(f\"훈련 파이프라인 실행 ARN: {training_execution.arn}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "monitoring-section",
   "metadata": {},
   "source": [
    "## 5. 모니터링 및 관리\n",
    "\n",
    "파이프라인 실행 상태를 모니터링하고 관리하는 방법을 제공합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "monitoring-functions",
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_pipeline_status(execution_arn):\n",
    "    \"\"\"\n",
    "    파이프라인 실행 상태 확인\n",
    "    \"\"\"\n",
    "    sagemaker_client = boto3.client('sagemaker')\n",
    "    \n",
    "    response = sagemaker_client.describe_pipeline_execution(\n",
    "        PipelineExecutionArn=execution_arn\n",
    "    )\n",
    "    \n",
    "    return {\n",
    "        'status': response['PipelineExecutionStatus'],\n",
    "        'start_time': response.get('CreationTime'),\n",
    "        'end_time': response.get('LastModifiedTime')\n",
    "    }\n",
    "\n",
    "def list_recent_executions(pipeline_name, max_results=10):\n",
    "    \"\"\"\n",
    "    최근 파이프라인 실행 목록 조회\n",
    "    \"\"\"\n",
    "    sagemaker_client = boto3.client('sagemaker')\n",
    "    \n",
    "    response = sagemaker_client.list_pipeline_executions(\n",
    "        PipelineName=pipeline_name,\n",
    "        MaxResults=max_results\n",
    "    )\n",
    "    \n",
    "    return response['PipelineExecutionSummaries']\n",
    "\n",
    "# 실행 상태 확인 예제\n",
    "print(\"배치 추론 파이프라인 상태:\")\n",
    "batch_status = check_pipeline_status(batch_execution.arn)\n",
    "print(batch_status)\n",
    "\n",
    "print(\"\\n훈련 파이프라인 상태:\")\n",
    "training_status = check_pipeline_status(training_execution.arn)\n",
    "print(training_status)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cleanup-section",
   "metadata": {},
   "source": [
    "## 6. 정리 및 다음 단계\n",
    "\n",
    "### 구현된 기능\n",
    "1. **Batch Inference Pipeline**: S3 업로드 → 전처리 → 배치 추론\n",
    "2. **Training Pipeline**: 전처리 → 훈련 → 평가 → 모델 등록\n",
    "3. **EventBridge 통합**: S3 이벤트 기반 자동 실행 (설정 가이드 제공)\n",
    "4. **모니터링**: 파이프라인 실행 상태 추적\n",
    "\n",
    "### EventBridge 설정 가이드\n",
    "EventBridge 규칙을 완전히 설정하려면 다음 단계를 수행하세요:\n",
    "\n",
    "1. **S3 버킷 이벤트 알림 설정**\n",
    "2. **Lambda 함수 배포** (파이프라인 트리거용)\n",
    "3. **EventBridge 규칙 생성**\n",
    "4. **IAM 권한 설정**\n",
    "\n",
    "### 다음 단계\n",
    "1. **알림 시스템**: SNS를 통한 파이프라인 완료 알림\n",
    "2. **에러 처리**: 실패 시 재시도 로직 및 알림\n",
    "3. **비용 최적화**: 스팟 인스턴스 활용\n",
    "4. **보안 강화**: VPC 엔드포인트 및 암호화 적용\n",
    "5. **A/B 테스트**: 모델 성능 비교 자동화\n",
    "\n",
    "### 주의사항\n",
    "- EventBridge 규칙과 Lambda 함수는 별도로 배포해야 합니다\n",
    "- 적절한 IAM 권한 설정이 필요합니다\n",
    "- 비용 모니터링을 위해 CloudWatch 대시보드를 설정하세요\n",
    "- 파이프라인 실행 전에 필요한 모델 아티팩트가 S3에 있는지 확인하세요"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
